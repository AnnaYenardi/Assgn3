{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3da434a8",
   "metadata": {},
   "source": [
    "# Assignment 3: Introduction to Neural Networks using PyTorch\n",
    "\n",
    "It consists of three questions: Q1 and Q2 are mandatory. Q3 is for self-learning and submission of the solution for this question is optional. \n",
    "\n",
    "**Q1:** Approximating the function |x-1| using a neural network.\n",
    "\n",
    "**Q2:** Implementation of three-input MAJORITY gate. The truth-table for this gate is provided in the problem statement.\n",
    "\n",
    "**Q3:** (**optional**) Hand digit recognition using MLP. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afedfcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL FIRST\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f772a",
   "metadata": {},
   "source": [
    "# Q1 Function Approximation [5 marks] \n",
    "\n",
    "Building deep learning networks manually is challenging. \n",
    "\n",
    "PyTorch abstracts that away using the `nn.Linear(in_dimensions, out_dimensions)` layer that does this under the hood.  \n",
    "`Linear` represents a fully connected layer with bias automatically included.  \n",
    "If you do not wish to include a bias column, you can simply call `nn.Linear(in_dimensions, out_dimensions, bias=False)` instead.\n",
    "\n",
    "PyTorch has many other layers implemented for various model architectures.  \n",
    "You can read more in the glossary as well as in the docs: https://pytorch.org/docs/stable/nn.html  \n",
    "For now, we will only be using fully connected `Linear` layers.\n",
    "\n",
    "We inherit from PyTorch's `nn.Module` class to build the model `y = |x-1|`.  \n",
    "\n",
    "\n",
    "This should allow you to appreciate the ease in which we can build neural networks using PyTorch.  \n",
    "\n",
    "The `nn.Module` class is an interface that houses two main methods: `__init__`, where we instantiate our layers and activation functions, and `forward`, that performs the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Forward pass [2 marks]\n",
    "\n",
    "For this problem, NN architecture is given to you. Please refer the following figure.\n",
    "\n",
    "\n",
    "<img src=\"./img_toy_nn.jpg\" width=\"180\">  \n",
    "\n",
    "\n",
    "You need to implement the constructor method and `forward` method to simulate the above NN.\n",
    "\n",
    "For this, you need to select the parameters for the model such as number of inputs, number of perceptrons in each hidden layer, activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81145ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Absolute(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Absolute, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(2, 3)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897a701",
   "metadata": {},
   "source": [
    "### Q1.2 Backward propagation  [3 marks]\n",
    "\n",
    "Instantitate the model, select the optimizer, loss function, and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587d9e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "absolute_function = Absolute()\n",
    "# Select the optimizer (uncomment the next line and fill right hand side)\n",
    "# optimiser =          \n",
    "# Select the loss function (uncomment the next line and fill right hand side)                          \n",
    "#loss_fn = \n",
    "\n",
    "#Generation of training data\n",
    "x = torch.linspace(-10, 10, 1000).reshape(-1, 1)\n",
    "y = torch.abs(x-1)\n",
    "\n",
    "#Training the model (uncomment the next line and fill right hand side)\n",
    "#epochs = \n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs+1):\n",
    "    # reset gradients to 0\n",
    "    #your code goes here\n",
    "\n",
    "    # get predictions\n",
    "    #your code goes here\n",
    "    \n",
    "    # compute loss (uncomment the next line and fill right hand side)\n",
    "    # abs_loss =  \n",
    "\n",
    "    # backpropagate\n",
    "    #your code goes here\n",
    "    \n",
    "    # update the model weights\n",
    "    #your code goes here\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print (f\"{i:5d}\", abs_loss.item(), sep='\\t')\n",
    "\n",
    "y_pred = absolute_function(x)\n",
    "plt.plot(x, y, linestyle='solid', label='|x-1|')\n",
    "plt.plot(x, y_pred.detach().numpy(), linestyle='dashed', label='perceptron')\n",
    "plt.axis('equal')\n",
    "plt.title('Fit NN on y=|x-1| function')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Modeling of MAJORITY gate using a minimalist neural network [5 marks]\n",
    "\n",
    "The objective is to model the three-input MAJORITY gate  using a neural network. \n",
    "\n",
    "As the name implies, the output is majority of the inputs. \n",
    "    \n",
    "    If the number of 1s in the input are greater than number of 0s, then output is 1. Otherwise, output is 0.\n",
    "    \n",
    "Truth table of the three-input MAJORITY gate is provided below. \n",
    "\n",
    "```\n",
    "| x1 | x2 | x3| y|\n",
    "|---------|---|--|\n",
    "| 0  | 0  | 0 | 0| \n",
    "| 0  | 0  | 1 | 0|\n",
    "| 0  | 1  | 0 | 0|\n",
    "| 0  | 1  | 1 | 1|\n",
    "| 1  | 0  | 0 | 0| \n",
    "| 1  | 0  | 1 | 1|\n",
    "| 1  | 1  | 0 | 1|\n",
    "| 1  | 1  | 1 | 1|\n",
    "\n",
    "```\n",
    "\n",
    "Unlike Q1, we did not provide network architecture. You should use least possible number of neurons and layers for this problem\n",
    "\n",
    "**Hint:** Check if you can skip hidden layers? If you cannot avoid hidden layer, select least number of neuron(s) in the hidden layer(s) that mininizes the prediction error. Explain the rationale behind the inclusion/exclusion of hidden layer(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.1 Forward Pass [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective is to model the majority gate with LEAST POSSIBLE number of perceptrons/neurons and layers\n",
    "class MajorityGate(nn.Module):  \n",
    "    def __init__(self): # set the arguments you'd need, including activation function (remember LEAST POSSIBLE number of perceptrons)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 Backward Propagation [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_gate = MajorityGate()\n",
    "# Select the optimizer (uncomment the next line and fill right hand side)\n",
    "# optimiser =          \n",
    "# Select the loss function (uncomment the next line and fill right hand side)                          \n",
    "#loss_fn = \n",
    "\n",
    "#Generation of training data (uncomment the next line and fill right hand side)\n",
    "#x = \n",
    "#y = \n",
    "\n",
    "#Training the model \n",
    "#uncomment the next line and fill right hand side\n",
    "#epochs = \n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs+1):\n",
    "    # reset gradients to 0\n",
    "    #your code goes here\n",
    "\n",
    "    # get predictions\n",
    "    #your code goes here\n",
    "    \n",
    "    # compute loss (uncomment the next line and fill right hand side)\n",
    "    # loss_gate = \n",
    "\n",
    "    # backpropagate \n",
    "    #your code goes here\n",
    "    \n",
    "    # update the model weights\n",
    "    #your code goes here\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print (f\"{i:5d}\", loss_gate.item(), sep='\\t')\n",
    "\n",
    "y_pred = majority_gate(x)\n",
    "#Output of the neural network\n",
    "print(y_pred.round())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151f959",
   "metadata": {},
   "source": [
    "### Q3  [Optinal] Using NN to recognize handwritten digits [0 marks] (only for self practice; no need to submit the solution to this question)\n",
    "\n",
    "\n",
    "In the final part of this problem set, we will be building a neural network to classify images to their respective digits.  \n",
    "\n",
    "You will build and train a model on the classic **MNIST Handwritten Digits** dataset. Each grayscale image is a $28 \\times 28$ matrix/tensor that looks like so:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" width=\"500\" />\n",
    "\n",
    "MNIST is a classification problem and the task is to take in an input image and classify them into one of ten buckets: the digits from $0$ to $9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda667dc",
   "metadata": {},
   "source": [
    "### Loading an external dataset\n",
    "\n",
    "The cell below imports the MNIST dataset, which is already pre-split into train and test sets.  \n",
    "\n",
    "The download takes approximately 63MB of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce62735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL – THIS DOWNLOADS THE MNIST DATASET\n",
    "# RUN THIS CELL BEFORE YOU RUN THE REST OF THE CELLS BELOW\n",
    "#Install torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "# This downloads the MNIST datasets ~63MB\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True)\n",
    "mnist_test  = datasets.MNIST(\"./\", train=False, download=True)\n",
    "\n",
    "x_train = mnist_train.data.reshape(-1, 784) / 255\n",
    "y_train = mnist_train.targets\n",
    "    \n",
    "x_test = mnist_test.data.reshape(-1, 784) / 255\n",
    "y_test = mnist_test.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092f6c4",
   "metadata": {},
   "source": [
    "### 3.1 - Define the model architechure and implement the forward pass\n",
    "Create a 3-layer network in the `__init__` method of the model `DigitNet`.  \n",
    "These layers are all `Linear` layers and should correspond to the following the architecture:\n",
    "\n",
    "<img src=\"img_linear_nn.png\" width=\"600\">\n",
    "\n",
    "In our data, a given image $x$ has been flattened from a 28x28 image to a 784-length array.\n",
    "\n",
    "After initializing the layers, stitch them together in the `forward` method. Your network should look like so:\n",
    "\n",
    "$$x \\rightarrow \\text{Linear(512)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(128)} \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear(10)} \\rightarrow \\text{Softmax} \\rightarrow \\hat{y}$$\n",
    "\n",
    "**Softmax Layer**: The final softmax activation is commonly used for classification tasks, as it will normalizes the results into a vector of values that follows a probability distribution whose total sums up to 1. The output values are between the range [0,1] which is nice because we are able to avoid binary classification and accommodate as many classes or dimensions in our neural network model.\n",
    "\n",
    "*Note: When using `torch.softmax(...)` on the final layer, ensure you are applying it on the correct dimension (as you did in NumPy via the `axis` argument in popular methods)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596d04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitNet(nn.Module):\n",
    "    def __init__(self, input_dimensions, num_classes): # set the arguments you'd need\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        \n",
    "        - your network should work for any input and output size \n",
    "            – add appropriate arguments in the object constructor\n",
    "        - create the 3 layers (and a ReLU layer) using the torch.nn layers API\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the network.\n",
    "        \n",
    "        PARAMS:\n",
    "            x : the input tensor (batch size is the entire dataset)\n",
    "            \n",
    "        RETURNS\n",
    "            the output of the entire 3-layer model\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        YOUR CODE\n",
    "        \n",
    "        - pass the inputs through the sequence of layers\n",
    "        - run the final output through the Softmax function on the right dimension!\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356b9ad",
   "metadata": {},
   "source": [
    "### Q3.2 Training Loop\n",
    "\n",
    "As demonstrated in Section 3.2, implement the function `train_model` that performs the following for every epoch/iteration:\n",
    "\n",
    "1. set the optimizer's gradients to zero\n",
    "2. forward pass\n",
    "3. calculate the loss\n",
    "4. backpropagate using the loss\n",
    "5. take an optimzer step to update weights\n",
    "\n",
    "This time, use the Adam optimiser to train the network.  \n",
    "Use Cross-Entropy Loss, since we are performing a classification.  \n",
    "Train for 20 epochs.  \n",
    "\n",
    "*Note: refer to the command glossary to find out how to instantiate optimisers, losses, and more*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, epochs=20):\n",
    "    \"\"\"\n",
    "    Trains the model for 20 epochs/iterations\n",
    "    \n",
    "    PARAMS:\n",
    "        x_train : a tensor of training features of shape (60000, 784)\n",
    "        y_train : a tensor of training labels of shape (60000, 1)\n",
    "        epochs  : number of epochs, default of 20\n",
    "        \n",
    "    RETURNS:\n",
    "        the final model \n",
    "    \"\"\"\n",
    "    model = DigitNet(784, 10)\n",
    "    optimiser = ... # use Adam\n",
    "    loss_fn = ...   # use cross-entropy loss\n",
    "\n",
    "    for i in range(epochs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model\n",
    "                \n",
    "digit_model = train_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fdee35",
   "metadata": {},
   "source": [
    "### Q3.3 - Explore your model\n",
    "\n",
    "Now that we have trained the model, let us run some predictions on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a demonstration: You can use this cell for exploring your trained model\n",
    "\n",
    "idx = 0 # try on some index\n",
    "\n",
    "scores = digit_model(x_test[idx:idx+1])\n",
    "_, predictions = torch.max(scores, 1)\n",
    "print(\"true label:\", y_test[idx].item())\n",
    "print(\"pred label:\", predictions[0].item())\n",
    "\n",
    "plt.imshow(x_test[idx].numpy().reshape(28, 28), cmap='gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc94586",
   "metadata": {},
   "source": [
    "### Q3.4 - Evaluate the model\n",
    "\n",
    "Now that we have trained the model, we should evaluate it using our test set.  \n",
    "We will be using the accuracy (whether or not the model predicted the correct label) to measure the model performance.  \n",
    "\n",
    "Since our model takes in a (n x 784) tensor and returns a (n x 10) tensor of probability scores for each of the 10 classes, we need to convert the probability scores into the actual predictions by taking the index of the maximum probability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5684246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(scores, labels):\n",
    "    \"\"\"\n",
    "    helper function that returns accuracy of model (out of 100%)\n",
    "    PARAMS:\n",
    "        scores : the raw softmax scores of the network\n",
    "        label : the ground truth labels\n",
    "        \n",
    "    RETURNS:\n",
    "        accuracy out of 100%\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "scores = digit_model(x_test) # n x 10 tensor\n",
    "get_accuracy(scores, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
